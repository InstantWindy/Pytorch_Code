{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 显示和处理图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as tfs\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # 设置画图的尺寸\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def show_images(images): # 定义画图工具\n",
    "    images = np.reshape(images, [images.shape[0], -1])\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return \n",
    "\n",
    "def preprocess_img(x):\n",
    "    x = tfs.ToTensor()(x)\n",
    "    return (x - 0.5) / 0.5\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义采样函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 数据加载\n",
    "\n",
    "在PyTorch中，数据加载可通过自定义的数据集对象。数据集对象被抽象为`Dataset`类，实现自定义的数据集需要继承Dataset，并实现两个Python魔法方法：\n",
    "- `__getitem__`：返回一条数据，或一个样本。`obj[index]`等价于`obj.__getitem__(index)`\n",
    "- `__len__`：返回样本的数量。`len(obj)`等价于`obj.__len__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler): # 定义一个取样的函数\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 50000\n",
    "NUM_VAL = 5000\n",
    "\n",
    "NOISE_DIM = 96\n",
    "batch_size = 128\n",
    "\n",
    "train_set = MNIST('./mnist', train=True, download=True, transform=preprocess_img)\n",
    "\n",
    "train_data = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "val_set = MNIST('./mnist', train=True, download=True, transform=preprocess_img)\n",
    "\n",
    "val_data = DataLoader(val_set, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "\n",
    "imgs = deprocess_img(train_data.__iter__().next()[0].view(batch_size, 784)).numpy().squeeze() # 可视化图片效果\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader的函数定义如下： \n",
    "`DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)`\n",
    "\n",
    "- dataset：加载的数据集(Dataset对象)\n",
    "- batch_size：batch size\n",
    "- shuffle:：是否将数据打乱\n",
    "- sampler： 样本抽样，后续会详细介绍\n",
    "- num_workers：使用多进程加载的进程数，0代表不使用多进程\n",
    "- collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可\n",
    "- pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些\n",
    "- drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader是一个可迭代的对象，意味着我们可以像使用迭代器一样使用它，例如：\n",
    "```python\n",
    "for batch_datas, batch_labels in dataloader:\n",
    "    train()\n",
    "```\n",
    "或\n",
    "```\n",
    "dataiter = iter(dataloader)\n",
    "batch_datas, batch_labesl = next(dataiter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据处理中，有时会出现某个样本无法读取等问题，比如某张图片损坏。这时在`__getitem__`函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。如果实在是遇到这种情况无法处理，则可以返回None对象，然后在`Dataloader`中实现自定义的`collate_fn`，将空对象过滤掉。但要注意，在这种情况下dataloader返回的batch数目会少于batch_size。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import  Image\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(224), # 缩放图片(Image)，保持长宽比不变，最短边为224像素\n",
    "    T.CenterCrop(224), # 从图片中间切出224*224的图片\n",
    "    T.ToTensor(), # 将图片(Image)转成Tensor，归一化至[0, 1]\n",
    "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]) # 标准化至[-1, 1]，规定均值和标准差\n",
    "])\n",
    "\n",
    "class DogCat(data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        imgs = os.listdir(root)\n",
    "        self.imgs = [os.path.join(root, img) for img in imgs]\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        label = 0 if 'dog' in img_path.split('/')[-1] else 1\n",
    "        data = Image.open(img_path)\n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewDogCat(DogCat): # 继承前面实现的DogCat数据集\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            # 调用父类的获取函数，即 DogCat.__getitem__(self, index)\n",
    "            return super(NewDogCat,self).__getitem__(index)\n",
    "        except:\n",
    "            return None, None\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate # 导入默认的拼接方式\n",
    "def my_collate_fn(batch):\n",
    "    '''\n",
    "    batch中每个元素形如(data, label)\n",
    "    '''\n",
    "    # 过滤为None的数据\n",
    "    batch = list(filter(lambda x:x[0] is not None, batch))\n",
    "    if len(batch) == 0: return t.Tensor()\n",
    "    return default_collate(batch) # 用默认方式拼接过滤后的batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = NewDogCat('data/dogcat_wrong/', transforms=transform)\n",
    "dataloader = DataLoader(dataset, 2, collate_fn=my_collate_fn, num_workers=1,shuffle=True)\n",
    "for batch_datas, batch_labels in dataloader:\n",
    "    print(batch_datas.size(),batch_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([2, 3, 224, 224]) torch.Size([2])\n",
    "\n",
    "torch.Size([2, 3, 224, 224]) torch.Size([2])\n",
    "\n",
    "torch.Size([1, 3, 224, 224]) torch.Size([1])\n",
    "\n",
    "torch.Size([2, 3, 224, 224]) torch.Size([2])\n",
    "\n",
    "torch.Size([1, 3, 224, 224]) torch.Size([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来看一下上述batch_size的大小。其中第2个的batch_size为1，这是因为有一张图片损坏，导致其无法正常返回。而最后1个的batch_size也为1，这是因为共有9张（包括损坏的文件）图片，无法整除2（batch_size），因此最后一个batch的数据会少于batch_szie，可通过指定`drop_last=True`来丢弃最后一个不足batch_size的batch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于诸如样本损坏或数据集加载异常等情况，还可以通过其它方式解决。例如但凡遇到异常情况，就随机取一张图片代替：\n",
    "```python\n",
    "class NewDogCat(DogCat):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return super(NewDogCat, self).__getitem__(index)\n",
    "        except:\n",
    "            new_index = random.randint(0, len(self)-1)\n",
    "            return self[new_index]\n",
    "```\n",
    "相比较丢弃异常图片而言，这种做法会更好一些，因为它能保证每个batch的数目仍是batch_size。但在大多数情况下，最好的方式还是对数据进行彻底清洗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader里面并没有太多的魔法方法，它封装了Python的标准库`multiprocessing`，使其能够实现多进程加速。在此提几点关于Dataset和DataLoader使用方面的建议：\n",
    "1. 高负载的操作放在`__getitem__`中，如加载图片等。\n",
    "2. dataset中应尽量只包含只读对象，避免修改任何可变对象，利用多线程进行操作。\n",
    "\n",
    "第一点是因为多进程会并行的调用`__getitem__`函数，将负载高的放在`__getitem__`函数中能够实现并行加速。\n",
    "第二点是因为dataloader使用多进程加载，如果在`Dataset`实现中使用了可变对象，可能会有意想不到的冲突。在多线程/多进程中，修改一个可变对象，需要加锁，但是dataloader的设计使得其很难加锁（在实际使用中也应尽量避免锁的存在），因此最好避免在dataset中修改可变对象。例如下面就是一个不好的例子，在多进程处理中`self.num`可能与预期不符，这种问题不会报错，因此难以发现。如果一定要修改可变对象，建议使用Python标准库`Queue`中的相关数据结构。\n",
    "\n",
    "```python\n",
    "class BadDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.datas = range(100)\n",
    "        self.num = 0 # 取数据的次数\n",
    "    def __getitem__(self, index):\n",
    "        self.num += 1\n",
    "        return self.datas[index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Python `multiprocessing`库的另一个问题是，在使用多进程时，如果主程序异常终止（比如用Ctrl+C强行退出），相应的数据加载进程可能无法正常退出。这时你可能会发现程序已经退出了，但GPU显存和内存依旧被占用着，或通过`top`、`ps aux`依旧能够看到已经退出的程序，这时就需要手动强行杀掉进程。建议使用如下命令：\n",
    "\n",
    "```\n",
    "ps x | grep <cmdline> | awk '{print $1}' | xargs kill\n",
    "```\n",
    "\n",
    "- `ps x`：获取当前用户的所有进程\n",
    "- `grep <cmdline>`：找到已经停止的PyTorch程序的进程，例如你是通过python train.py启动的，那你就需要写`grep 'python train.py'`\n",
    "- `awk '{print $1}'`：获取进程的pid\n",
    "- `xargs kill`：杀掉进程，根据需要可能要写成`xargs kill -9`强制杀掉进程\n",
    "\n",
    "在执行这句命令之前，建议先打印确认一下是否会误杀其它进程\n",
    "```\n",
    "ps x | grep <cmdline> | ps x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中还单独提供了一个`sampler`模块，用来对数据进行采样。常用的有随机采样器：`RandomSampler`，当dataloader的`shuffle`参数为True时，系统会自动调用这个采样器，实现打乱数据。默认的是采用`SequentialSampler`，它会按顺序一个一个进行采样。这里介绍另外一个很有用的采样方法：\n",
    "`WeightedRandomSampler`，它会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它来进行重采样。\n",
    "\n",
    "构建`WeightedRandomSampler`时需提供两个参数：每个样本的权重`weights`、共选取的样本总数`num_samples`，以及一个可选参数`replacement`。权重越大的样本被选中的概率越大，待选取的样本数目一般小于全部的样本数目。`replacement`用于指定是否可以重复选取某一个样本，默认为True，即允许在一个epoch中重复采样某一个数据。如果设为False，则当某一类的样本被全部选取完，但其样本数目仍未达到num_samples时，sampler将不会再从该类中选择数据，此时可能导致`weights`参数失效。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = DogCat('data/dogcat/', transforms=transform)\n",
    "\n",
    "# 狗的图片被取出的概率是猫的概率的两倍\n",
    "# 两类图片被取出的概率与weights的绝对大小无关，只和比值有关\n",
    "weights = [2 if label == 1 else 1 for data, label in dataset]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import  WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights,\\\n",
    "                                num_samples=9,\\\n",
    "                                replacement=True)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=3,\n",
    "                        sampler=sampler)\n",
    "for datas, labels in dataloader:\n",
    "    print(labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见猫狗样本比例约为1:2，另外一共只有8个样本，但是却返回了9个，说明肯定有被重复返回的，这就是replacement参数的作用，下面将replacement设为False试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(weights, 8, replacement=False)\n",
    "dataloader = DataLoader(dataset, batch_size=4, sampler=sampler)\n",
    "for datas, labels in dataloader:\n",
    "    print(labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下，num_samples等于dataset的样本总数，为了不重复选取，sampler会将每个样本都返回，这样就失去weight参数的意义了。\n",
    "\n",
    "从上面的例子可见sampler在样本采样中的作用：如果指定了sampler，shuffle将不再生效，并且sampler.num_samples会覆盖dataset的实际大小，即一个epoch返回的图片总数取决于`sampler.num_samples`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
